{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final-lab-challenge.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohamedsgap/practice-with-pytorch/blob/master/final_lab_challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "BhihHWLsDAVO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5de94a11-5d98-4519-a2f6-1b72a4ad04bd"
      },
      "cell_type": "code",
      "source": [
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x5770a000 @  0x7f4e7b5992a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sxCtFrmbDbEO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "6d07f52f-7630-4419-ddf0-ec52a048184d"
      },
      "cell_type": "code",
      "source": [
        "# we need pillow version of 5.3.0\n",
        "# we will uninstall the older version first\n",
        "!pip uninstall -y Pillow\n",
        "# install the new one\n",
        "!pip install Pillow==5.3.0\n",
        "# import the new one\n",
        "import PIL\n",
        "print(PIL.PILLOW_VERSION)\n",
        "# this should print 5.3.0. If it doesn't, then restart your runtime:\n",
        "# Menu > Runtime > Restart Runtime"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling Pillow-5.4.0:\n",
            "  Successfully uninstalled Pillow-5.4.0\n",
            "Collecting Pillow==5.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 15.1MB/s \n",
            "\u001b[?25hInstalling collected packages: Pillow\n",
            "Successfully installed Pillow-5.3.0\n",
            "4.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OaMS6X0WDevG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# we will download the required data files\n",
        "!wget -cq https://github.com/udacity/pytorch_challenge/raw/master/cat_to_name.json\n",
        "!wget -cq https://s3.amazonaws.com/content.udacity-data.com/courses/nd188/flower_data.zip\n",
        "#!rm -r flower_data || true\n",
        "!unzip -qq flower_data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d38DcepVDfbK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# we will download the required data files\n",
        "!wget -cq https://github.com/udacity/pytorch_challenge/raw/master/cat_to_name.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ry3LSp8GDn-Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Imports here\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import json\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from collections import OrderedDict\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I2W3aOouDoyK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_dir = '/flower_data'\n",
        "train_dir = data_dir + '/train'\n",
        "valid_dir = data_dir + '/valid'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_It06gpqD1ms",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO: Define your transforms for the training and validation sets\n",
        "data_transforms =  {\n",
        "    'train': torchvision.transforms.Compose([\n",
        "        torchvision.transforms.RandomResizedCrop(224),\n",
        "        torchvision.transforms.ColorJitter(hue=.06, saturation=.05),\n",
        "        torchvision.transforms.RandomHorizontalFlip(),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ]),\n",
        "    'valid': torchvision.transforms.Compose([        \n",
        "        torchvision.transforms.Resize(224),\n",
        "        torchvision.transforms.CenterCrop(224),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ])\n",
        "}\n",
        "\n",
        "# TODO: Load the datasets with ImageFolder\n",
        "image_datasets = {\n",
        "    'train': torchvision.datasets.ImageFolder(\n",
        "        'flower_data/train',\n",
        "        transform=data_transforms['train']\n",
        "    ),\n",
        "    'valid': torchvision.datasets.ImageFolder(\n",
        "        'flower_data/valid',\n",
        "        transform=data_transforms['valid']\n",
        "    )\n",
        "}\n",
        "\n",
        "# TODO: Using the image datasets and the trainforms, define the dataloaders\n",
        "dataloaders =  {\n",
        "    'train': torch.utils.data.DataLoader(\n",
        "        image_datasets['train'],\n",
        "        batch_size=256,\n",
        "        shuffle=True,\n",
        "        num_workers=16\n",
        "    ),\n",
        "    'valid': torch.utils.data.DataLoader(\n",
        "        image_datasets['valid'],\n",
        "        batch_size=256,\n",
        "        shuffle=True,\n",
        "        num_workers=16\n",
        "    )\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "na8_QTgmD2FV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('cat_to_name.json', 'r') as f:\n",
        "    cat_to_name = json.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6QQ8c6u4D3CP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO: Build and train your network\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j1e4TDwCD3oD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_model(device=torch.device('cpu')):\n",
        "    model = torchvision.models.resnet152(pretrained=True)\n",
        "    \n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    \n",
        "    num_features = model.fc.in_features\n",
        "    \n",
        "    fc_layers = torch.nn.Sequential(OrderedDict([\n",
        "        ('fc1', torch.nn.Linear(num_features, 1024)),\n",
        "        ('relu1', torch.nn.ReLU()),\n",
        "        ('dropout1', torch.nn.Dropout(p=0.4)),\n",
        "        ('fc2', torch.nn.Linear(1024, 512)),\n",
        "        ('relu2', torch.nn.ReLU()),\n",
        "        ('dropout2', torch.nn.Dropout(p=0.4)),\n",
        "        ('fc3', torch.nn.Linear(512, 256)),\n",
        "        ('relu3', torch.nn.ReLU()),\n",
        "        ('dropout3', torch.nn.Dropout(p=0.4)),\n",
        "        ('fc4', torch.nn.Linear(256, 102)),\n",
        "        ('output', torch.nn.LogSoftmax(dim=1))\n",
        "    ]))\n",
        "    model.fc = fc_layers\n",
        "    \n",
        "    model.to(device)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gUyyzxdYEOaa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "30305d16-154b-41c8-c2e8-0d4517a6903c"
      },
      "cell_type": "code",
      "source": [
        "model = create_model(device)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.torch/models/resnet152-b121ed2d.pth\n",
            "100%|██████████| 241530880/241530880 [00:06<00:00, 35650280.02it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "VL8CWgqlEO8b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(densenet, criterion, optimizer, num_epochs):\n",
        "    perfs = []\n",
        "    \n",
        "    best_loss = +np.inf\n",
        "    dataset_sizes = {\n",
        "        'train': len(image_datasets['train']),\n",
        "        'valid': len(image_datasets['valid'])\n",
        "    }\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        train_epoch_loss = 0\n",
        "        train_epoch_acc = 0\n",
        "        valid_epoch_loss = 0\n",
        "        valid_epoch_acc = 0\n",
        "        for phase in ['train', 'valid']:\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            for i, (inputs, labels) in enumerate(dataloaders[phase]):\n",
        "                if phase == 'train':\n",
        "                    densenet.train(True)\n",
        "                else:\n",
        "                    densenet.train(False)\n",
        "                \n",
        "                inputs = torch.autograd.Variable(inputs.to(device=device))\n",
        "                labels = torch.autograd.Variable(labels.to(device=device))\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs.data, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "                \n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                \n",
        "                running_loss += loss.item()\n",
        "                running_corrects += torch.sum(preds == labels.data).item()\n",
        "                \n",
        "                del inputs, labels, outputs, preds\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "            if phase == 'train':\n",
        "                train_epoch_loss = running_loss / dataset_sizes[phase]\n",
        "                train_epoch_acc = running_corrects / dataset_sizes[phase]\n",
        "            else:\n",
        "                valid_epoch_loss = running_loss / dataset_sizes[phase]\n",
        "                valid_epoch_acc = running_corrects / dataset_sizes[phase]\n",
        "        \n",
        "        print('Epoch [{}/{}] train loss: {:.6f} acc: {:.6f} valid loss: {:.6f} acc: {:.6f}'.format(\n",
        "            epoch + 1, num_epochs, train_epoch_loss, train_epoch_acc, valid_epoch_loss, valid_epoch_acc))\n",
        "        \n",
        "        perfs.append([train_epoch_loss, train_epoch_acc, valid_epoch_loss, valid_epoch_acc])\n",
        "        \n",
        "        if valid_epoch_loss < best_loss:\n",
        "            print(\"Model improved (from loss: {:.6f} to {:.6f}), saving \".format(best_loss, valid_epoch_loss))\n",
        "            best_loss = valid_epoch_loss\n",
        "            torch.save({\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'class_to_idx': image_datasets['train'].class_to_idx\n",
        "            }, 'checkpoint.pth.tar')\n",
        "    \n",
        "    print(\"Training completed - Best loss: {:.6f}\".format(best_loss))\n",
        "    perfs_df = pd.DataFrame(perfs, columns=['train_epoch_loss',\n",
        "                                            'train_epoch_acc',\n",
        "                                            'valid_epoch_loss',\n",
        "                                            'valid_epoch_acc'])\n",
        "    return perfs_df\n",
        "               "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tYUC1Dr1EPkN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.NLLLoss()\n",
        "optimizer = torch.optim.Adadelta(model.fc.parameters(), lr=1.1, rho=0.8, eps=1e-07, weight_decay=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1yLjt56kEbor",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "outputId": "7aef0d98-b757-44c7-bac9-6239688bc381"
      },
      "cell_type": "code",
      "source": [
        "num_epochs = 200\n",
        "perfs_df = train_model(model, criterion, optimizer, num_epochs)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-7ac85b9840f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mperfs_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-219bb898162e>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(densenet, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mrunning_corrects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                     \u001b[0mdensenet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\", line 101, in __getitem__\n    sample = self.loader(path)\n  File \"/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\", line 147, in default_loader\n    return pil_loader(path)\n  File \"/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\", line 129, in pil_loader\n    img = Image.open(f)\n  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 2321, in open\n  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 370, in preinit\n    def preinit():\n  File \"/usr/local/lib/python3.6/dist-packages/PIL/PpmImagePlugin.py\", line 158, in <module>\n    Image.register_extensions(PpmImageFile.format, [\".pbm\", \".pgm\", \".ppm\"])\nAttributeError: module 'PIL.Image' has no attribute 'register_extensions'\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "wvTKBsPlEcFg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "outputId": "ef9c164e-58f8-4e78-87c0-3f3dec18c7f2"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "plt.plot(np.arange(0, num_epochs), perfs_df['train_epoch_loss'], label='Train loss')\n",
        "plt.plot(np.arange(0, num_epochs), perfs_df['train_epoch_acc'], label='Train accuracy')\n",
        "plt.plot(np.arange(0, num_epochs), perfs_df['valid_epoch_loss'], label='Test loss')\n",
        "plt.plot(np.arange(0, num_epochs), perfs_df['valid_epoch_acc'], label=' Test accuracy')\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"upper left\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-a0d1ff11ba7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperfs_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_epoch_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperfs_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_epoch_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperfs_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid_epoch_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Test loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'perfs_df' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fd5bb07b208>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "bp3dXc6GEcn9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0aadd57c-29fa-4a30-d145-c23e70898f7a"
      },
      "cell_type": "code",
      "source": [
        "# TODO: Save the checkpoint \n",
        "\"\"\"\"\n",
        "model.class_to_idx = train_data.class_to_idx\n",
        "model.cpu\n",
        "torch.save({'structure' :'densenet121',\n",
        "            'hidden_layer1':120,\n",
        "            'state_dict':model.state_dict(),\n",
        "            'class_to_idx':model.class_to_idx},\n",
        "            'checkpoint.pth')\n",
        "            \n",
        "            \"\"\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"\\nmodel.class_to_idx = train_data.class_to_idx\\nmodel.cpu\\ntorch.save({\\'structure\\' :\\'densenet121\\',\\n            \\'hidden_layer1\\':120,\\n            \\'state_dict\\':model.state_dict(),\\n            \\'class_to_idx\\':model.class_to_idx},\\n            \\'checkpoint.pth\\')\\n            \\n            '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "K-InfArcEdRo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO: Write a function that loads a checkpoint and rebuilds the model\n",
        "\n",
        "def load_checkpoint():\n",
        "    model = create_model()\n",
        "    optimizer = torch.optim.Adadelta(model.fc.parameters(), lr=1.1, rho=0.8, eps=1e-07, weight_decay=0)\n",
        "    checkpoint = torch.load('checkpoint.pth.tar')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.class_to_idx = checkpoint['class_to_idx']\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    return model, optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DrPpy2_4Ed3E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def process_image(image):\n",
        "    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n",
        "        returns an Numpy array\n",
        "    '''\n",
        "    \n",
        "    # TODO: Process a PIL image for use in a PyTorch model\n",
        "    \n",
        "    preprocess = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.Resize(224),\n",
        "        torchvision.transforms.CenterCrop(224),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ])\n",
        "    \n",
        "    img = Image.open(image)\n",
        "    img = preprocess(img)\n",
        "    \n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V0aJbtuOEea6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def imshow(image, ax=None, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "    \n",
        "    # PyTorch tensors assume the color channel is the first dimension\n",
        "    # but matplotlib assumes is the third dimension\n",
        "    image = image.numpy().transpose((1, 2, 0))\n",
        "    \n",
        "    # Undo preprocessing\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    image = std * image + mean\n",
        "    \n",
        "    # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\n",
        "    image = np.clip(image, 0, 1)\n",
        "    \n",
        "    ax.imshow(image)\n",
        "    \n",
        "    return ax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NHge4lA0EfAM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(image_path, model, topk=5):\n",
        "    ''' Predict the class (or classes) of an image using a trained deep learning model.\n",
        "    '''\n",
        "    \n",
        "    # TODO: Implement the code to predict the class from an image file\n",
        "    \n",
        "     \n",
        "    model.eval()\n",
        "    model.cpu()\n",
        "    \n",
        "    image = process_image(image_path)\n",
        "    image = image.unsqueeze(0)\n",
        "    image = torch.autograd.Variable(image)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        output = model.forward(image)\n",
        "        probs = torch.exp(output)\n",
        "        top_probs, top_labels = probs.topk(topk)\n",
        "    \n",
        "    return torch.Tensor.numpy(top_probs[0]).tolist(), torch.Tensor.numpy(top_labels[0]).tolist()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fz5MkRVqFMuQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO: Display an image along with the top 5 classes\n",
        "\n",
        "%matplotlib inline\n",
        "img_path = 'flower_data/valid/102/image_08014.jpg'\n",
        "\n",
        "model, optimizer = load_checkpoint()\n",
        "top_probs, top_labels = predict(img_path, model, topk=5)\n",
        "idx_to_class = {val: key for key, val in model.class_to_idx.items()}\n",
        "\n",
        "plt.barh([cat_to_name[idx_to_class[lab]] for lab in top_labels],\n",
        "        top_probs,\n",
        "        align='center', color='blue')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n",
        "\n",
        "imshow(process_image(img_path))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ztGUni2-FNUx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}